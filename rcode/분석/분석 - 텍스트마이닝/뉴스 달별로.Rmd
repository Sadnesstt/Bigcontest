# **EDA 뉴스 텍스트 마이닝**
```{r, eval=FALSE}
library(tidyverse)
library(rJava)
library(KoNLP)
library(tm)
useNIADic()
library(reshape2)
library(slam)
library(lda)
library(LDAvis)
library(topicmodels)
library(lsa)
```

#### 사전에 미세먼지 관련 단어 업데이트
```{r, eval=FALSE}
mergeUserDic(data.frame(c("피엠십","피엠이점오","초미세먼지","마스크팩","클렌","에어드레서","샘플","고객","차량이부제"),"ncn"))
```

#### 불러 들이기
```{r, eval=FALSE}
news <- read_rds('complete/news.rds')
```

#### 중복 제거
```{r, eval=FALSE}
news <- news[-which(duplicated(news$CONTENT)),]
```

#### na 제거
```{r, eval=FALSE}
news <- news[!is.na(news$CONTENT),]
```

#### DT 분리(기간별 키워드 뽑기 위해)
```{r, eval=FALSE}
news <- news %>% separate(DT, c("year","month","day"),sep=c(4,6),convert=T)
```

#### pm2.5와 pm10을 피엠이점오와 피엠십 치환(영어를 소문자로 치환 후)
```{r, eval=FALSE}
news$CONTENT <- tolower(news$CONTENT)
news$CONTENT <- str_replace_all(news$CONTENT,"pm 10","피엠십")
news$CONTENT <- str_replace_all(news$CONTENT,"pm 2.5","피엠이점오")
news$CONTENT <- str_replace_all(news$CONTENT,"pm10","피엠십")
news$CONTENT <- str_replace_all(news$CONTENT,"pm2.5","피엠이점오")
news$CONTENT <- str_replace_all(news$CONTENT,"pm 25","피엠이점오")
news$CONTENT <- str_replace_all(news$CONTENT,"pm25","피엠이점오")
```

#### 미세먼지 표현
```{r, eval=FALSE}
news$CONTENT <- str_replace_all(news$CONTENT,"미세먼지","미세먼지 ")
news$CONTENT <- str_replace_all(news$CONTENT,"미세 먼지","미세먼지")
news$CONTENT <- str_replace_all(news$CONTENT,"미먼","미세먼지")
news$CONTENT <- str_replace_all(news$CONTENT,"초 미세먼지","초미세먼지")
```
#### 차량2부제
```{r, eval=FALSE}
news$CONTENT <- str_replace_all(news$CONTENT,"차량 2부제","차량이부제 ")
news$CONTENT <- str_replace_all(news$CONTENT,"차량2부제","차량이부제")
news$CONTENT <- str_replace_all(news$CONTENT,"2부제","이부제")
news$CONTENT <- str_replace_all(news$CONTENT,"2 부제","이부제")
```

#### 마스크팩 표현
```{r, eval=FALSE}
news$CONTENT <- str_replace_all(news$CONTENT,"마스크 팩","마스크팩 ")
news$CONTENT <- str_replace_all(news$CONTENT,"팩","마스크팩")

write_rds(news, 'complete/news_preprocessed.rds')
```

#### news good or bad day 전처리
```{r, eval=FALSE}
forecast <- read.csv('pre/forecast.csv')
forecast <- forecast %>% group_by(DT) %>% summarise(F10=mean(F10,na.rm=T),F25=mean(F25,na.rm=T))
goodDT <- forecast %>% filter(F10<2 & F25<2) %>% select(DT)
goodDT <- as.vector(goodDT$DT)
goodDT <- as.character(goodDT)
badDT <- forecast %>% filter(F10>2 & F25>2) %>% select(DT)
badDT <- as.vector(badDT$DT)
badDT <- as.character(badDT)
news2 <- news %>% mutate(month2=ifelse(month>=10, month, paste0("0",month)),
                         day2=ifelse(day>=10, day, paste0("0",day))) %>% unite(DT, c(year,month2,day2),sep="")%>%select(DT,CONTENT)
news_good <- news2 %>% filter(DT %in% c(goodDT))
news_bad <- news2 %>% filter(DT %in% c(badDT))

write_rds(news_good, 'complete/news_good.rds')
write_rds(news_bad, 'complete/news_bad.rds')
```


#### 불용어 제거(txt파일) 출처 : https://www.ranks.nl/stopwords/korean
```{r, eval=FALSE}
stopwords <- c(readLines("pre/user_stopwords_news0823.txt"))

myvector <- function(i){
  subset <- news %>% filter(month == i) %>% select("CONTENT")
  subset_clean <- sapply(subset, function(contents) gsub("[^가-힣]"," ",contents))
  
  for(j in 1:length(stopwords)){
    subset_clean <- gsub(stopwords[j],"",subset_clean)
  }
  return(subset_clean)
}


subset_clean <- paste0("month_clean",1:12)
for (i in 1:12){
  assign(subset_clean[i], myvector(i))
}

```

#### news 코퍼스 및 TDM.tf 구축 
```{r, eval=FALSE}
Noun <- function(doc){
  d <- as.character(doc)
  d2 <- paste(SimplePos09(d))
  d3 <- str_match(d2,"([가-힣]+)/N")
  d4 <- d3[,2]
  d4[!is.na(d4)]
}

month1.cps <- VCorpus(VectorSource(month_clean1))
month1.cps <- tm_map(month1.cps, stripWhitespace)

month2.cps <- VCorpus(VectorSource(month_clean2))
month2.cps <- tm_map(month2.cps, stripWhitespace)

month3.cps <- VCorpus(VectorSource(month_clean3))
month3.cps <- tm_map(month3.cps, stripWhitespace)

month4.cps <- VCorpus(VectorSource(month_clean4))
month4.cps <- tm_map(month4.cps, stripWhitespace)

month5.cps <- VCorpus(VectorSource(month_clean5))
month5.cps <- tm_map(month5.cps, stripWhitespace)

month6.cps <- VCorpus(VectorSource(month_clean6))
month6.cps <- tm_map(month6.cps, stripWhitespace)

month7.cps <- VCorpus(VectorSource(month_clean7))
month7.cps <- tm_map(month7.cps, stripWhitespace)

month8.cps <- VCorpus(VectorSource(month_clean8))
month8.cps <- tm_map(month8.cps, stripWhitespace)

month9.cps <- VCorpus(VectorSource(month_clean9))
month9.cps <- tm_map(month9.cps, stripWhitespace)

month10.cps <- VCorpus(VectorSource(month_clean10))
month10.cps <- tm_map(month10.cps, stripWhitespace)

month11.cps <- VCorpus(VectorSource(month_clean11))
month11.cps <- tm_map(month11.cps, stripWhitespace)

month12.cps <- VCorpus(VectorSource(month_clean12))
month12.cps <- tm_map(month12.cps, stripWhitespace)
```

#### 월별 TDM 구축
```{r, eval=FALSE}
month1.TDM.tf <- TermDocumentMatrix(month1.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month1.TDM.tf, 'complete/month1.TDM.tf.rds')


month2.TDM.tf <- TermDocumentMatrix(month2.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month2.TDM.tf, 'complete/month2.TDM.tf.rds')


month3.TDM.tf <- TermDocumentMatrix(month3.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month3.TDM.tf, 'complete/month3.TDM.tf.rds')


month4.TDM.tf <- TermDocumentMatrix(month4.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month4.TDM.tf, 'complete/month4.TDM.tf.rds')

month5.TDM.tf <- TermDocumentMatrix(month5.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month5.TDM.tf, 'complete/month5.TDM.tf.rds')


month6.TDM.tf <- TermDocumentMatrix(month6.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month6.TDM.tf, 'complete/month6.TDM.tf.rds')


month7.TDM.tf <- TermDocumentMatrix(month7.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month7.TDM.tf, 'complete/month7.TDM.tf.rds')


month8.TDM.tf <- TermDocumentMatrix(month8.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month8.TDM.tf, 'complete/month8.TDM.tf.rds')

month9.TDM.tf <- TermDocumentMatrix(month9.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month1.TDM.tf, 'complete/month9.TDM.tf.rds')


month10.TDM.tf <- TermDocumentMatrix(month10.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month10.TDM.tf, 'complete/month10.TDM.tf.rds')


month11.TDM.tf <- TermDocumentMatrix(month11.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month11.TDM.tf, 'complete/month11.TDM.tf.rds')


month12.TDM.tf <- TermDocumentMatrix(month12.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
write_rds(month12.TDM.tf, 'complete/month12.TDM.tf.rds')
```


# **news_good/bad 텍스트마이닝**

# 불용어 제거(txt파일) 출처 : https://www.ranks.nl/stopwords/korean
```{r, eval=FALSE}
ngood <- news_good %>% select("CONTENT")
ngood_clean <- sapply(ngood, function(contents) gsub("[^가-힣]"," ",contents))

for(j in 1:length(stopwords)){
  ngood_clean <- gsub(stopwords[j],"",ngood_clean)
}

nbad <- news_bad %>% select("CONTENT")
nbad_clean <- sapply(nbad, function(contents) gsub("[^가-힣]"," ",contents))

for(j in 1:length(stopwords)){
  nbad_clean <- gsub(stopwords[j],"",nbad_clean)
}
```
#### 미세먼지 좋은날/ 나쁜날 TDM 구축
```{r, eval=FALSE}
ngood.cps <- VCorpus(VectorSource(ngood_clean))
ngood.cps <- tm_map(ngood.cps, stripWhitespace)

nbad.cps <- VCorpus(VectorSource(nbad_clean))
nbad.cps <- tm_map(nbad.cps, stripWhitespace)


ngood.TDM.tf <- TermDocumentMatrix(ngood.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
ngood.TDM.tf2 <- removeSparseTerms(ngood.TDM.tf,sparse=0.99)
write_rds(ngood.TDM.tf, 'complete/ngood.TDM.tf.rds')

nbad.TDM.tf <- TermDocumentMatrix(nbad.cps, control=list(tokenize=Noun,wordLengths=c(2,6)))
nbad.TDM.tf2 <- removeSparseTerms(nbad.TDM.tf,sparse=0.99)
write_rds(nbad.TDM.tf, 'complete/nbad.TDM.tf.rds')
```

